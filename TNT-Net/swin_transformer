import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class Mlp(nn.Module):
    """
    Multi-layer Perceptron (MLP) module.

    Args:
        in_features (int): Number of input features.
        hidden_features (int): Number of hidden features.
        out_features (int): Number of output features.
        act_layer (nn.Module, optional): Activation layer to be used. Default is GELU.
        drop (float, optional): Dropout rate. Default is 0.0.

    Attributes:
        fc1 (nn.Linear): First linear layer.
        act (nn.Module): Activation layer.
        fc2 (nn.Linear): Second linear layer.
        drop (nn.Dropout): Dropout layer.
    """

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        """
        Initialize the MLP module.

        Args:
            in_features (int): Number of input features.
            hidden_features (int, optional): Number of hidden features. If not provided, it is set to `in_features`.
            out_features (int, optional): Number of output features. If not provided, it is set to `in_features`.
            act_layer (nn.Module, optional): Activation layer to be used. Default is GELU.
            drop (float, optional): Dropout rate. Default is 0.0.
        """
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        """
        Forward pass of the MLP module.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        """
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class WindowAttention(nn.Module):
    """
    Window-based Multi-Head Self-Attention (W-MSA) module.

    Args:
        dim (int): Number of input and output features.
        window_size (int): Window size for W-MSA.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional): If True, add a learnable bias to the query, key, and value. Default is True.
        qk_scale (float, optional): Override the default scaling of the query and key values. Default is None.
        attn_drop (float, optional): Dropout ratio of the attention weights. Default is 0.0.
        proj_drop (float, optional): Dropout ratio of the output projection. Default is 0.0.

    Attributes:
        dim (int): Number of input and output features.
        window_size (int): Window size for W-MSA.
        num_heads (int): Number of attention heads.
        scale (float): Scaling factor for the query and key values.
        qkv (nn.Linear): Linear layer for generating query, key, and value tensors.
        attn_drop (nn.Dropout): Dropout layer for the attention weights.
        proj (nn.Linear): Linear layer for the output projection.
        proj_drop (nn.Dropout): Dropout layer for the output projection.
        relative_position_bias_table (torch.Tensor): Precomputed relative position bias for the attention calculation.
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        """
        Initialize the W-MSA module.

        Args:
            dim (int): Number of input and output features.
            window_size (int): Window size for W-MSA.
            num_heads (int): Number of attention heads.
            qkv_bias (bool, optional): If True, add a learnable bias to the query, key, and value. Default is True.
            qk_scale (float, optional): Override the default scaling of the query and key values. Default is None.
            attn_drop (float, optional): Dropout ratio of the attention weights. Default is 0.0.
            proj_drop (float, optional): Dropout ratio of the output projection. Default is 0.0.
        """
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # Define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))

        # Get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size)
        coords_w = torch.arange(self.window_size)
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size - 1
        relative_coords[:, :, 0] *= 2 * self.window_size - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Forward pass of the W-MSA module.

        Args:
            x (torch.Tensor): Input tensor with shape (B, N, C).
            mask (torch.Tensor, optional): Attention mask. Default is None.

        Returns:
            torch.Tensor: Output tensor with shape (B, N, C).
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # B, nH, N, C

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
